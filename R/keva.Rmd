---
title: An attempt at optimal allocation minimizing optorunity costs
author: Daniel R. Williams ^[Director of Quality Control, Greenstreet Growers, Lothian,
  MD] ^[PhD Candidate, Dept. of Ag. & Crop Science, Ohio State University, Columbus,  Ohio]
date: 7 Aug 2024
output:
  html_notebook: default
---

# To Do

- [ ] I/O functions
  - [ ] where/how best to pull data? `rest` or an api?
  - [ ] how best to manage data? local storage of downloaded data?
- [ ] Alogrithms
  - [ ] retrieve plan from previous projects
  - [ ] retrieve plan from Tim

```{r setup, echo=FALSE, error=TRUE, message=TRUE, warning=TRUE, include=FALSE}
# this block loads required packages and some custom functions. 
cran_pacs <- c("tidyverse","writexl","fs","stringr","rlang","magrittr","stats","foreach", "doParallel", "parallel", "fs", "janitor", "stringr","lubridate","forcats", "purrr","multidplyr", "curl") #"rvest", "chromote", "httr2"
dev_pacs <- c() 
bioc_pacs <- c()
py_pacs <- c()

# install installers
if(length(find.package("pak", quiet = TRUE))==0) install.packages("pak")
if(length(find.package("librarian", quiet = TRUE))==0) pak::pak("librarian")
installer_pacs <- Filter(librarian::check_installed, c(if (length(bioc_pacs) > 0) "BiocManager", if (length(py_pacs) > 0) "reticulate"))
if (length(installer_pacs) > 0) pak::pak(installer_pacs, upgrade = TRUE)

# install and load R pacs
r_pacs <- c(cran_pacs,dev_pacs,bioc_pacs) |> unique()
if (length(r_pacs) > 0) {
  pak::pak(r_pacs, upgrade = TRUE)
  librarian::shelf(r_pacs)
}

# install python packs
if (length(py_pacs)>0) {
  if (!reticulate::py_available(initialize = TRUE)) reticulate::install_python()
  if (reticulate::py_available(initialize = TRUE)) reticulate::py_install(py_pacs)
}

## register cores for foreach/doParallel
n_cores <- parallel::detectCores()
# registerDoParallel(cores=n_cores)
# stopImplicitCluster()

# doParallel cluster
if(!is_null(getDefaultCluster())) stopCluster(getDefaultCluster())
makeCluster(n_cores-1) |> setDefaultCluster()
registerDoParallel(getDefaultCluster())

# dplyr cluster
if(exists("dCluster")) rm(dCluster)
dCluster <- new_cluster(n_cores-1)
cluster_library_quiety <- purrr::quietly(cluster_library)
# cluster_library_quiety(dCluster, loadedNamespaces())
cluster_library_quiety(dCluster, librarian::check_attached())
```

```{r pullhistoricdata}

data_path <- "Dropbox (Personal)/Projects/investing" |> fs::path_home()
hist_path <- "Dropbox (Personal)/Projects/investing/historic data" |> fs::path_home()

# "https://query1.finance.yahoo.com/v7/finance/download/FAMRX?period1=938179800&period2=1723135605&interval=1d&events=history&includeAdjustedClose=true"

symbols <-  path(data_path,"fidelity_targets_20240803.tsv") |> 
  read_tsv(show_col_types = FALSE) |>
  pull(id)

baseURL <- "https://query1.finance.yahoo.com/v7/finance/download/"
# symbols <- c("FAMRX","FXAIX")
start_date <- "01-01-2010" |> dmy() |> as.integer()
end_date <- now() |> round_date() |> as.integer()
query <- str_c("?period1=",start_date,"&period2=",end_date,"&interval=1d&events=history&includeAdjustedClose=true")
url_query <- str_c(baseURL,symbols |> str_to_lower(),query)
file_dest <- path(hist_path, symbols |> str_c(".csv"))

multi_download(url_query, file_dest)

```

```{r currentdata}



```

